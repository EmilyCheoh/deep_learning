(this["webpackJsonpreact-practice-app"]=this["webpackJsonpreact-practice-app"]||[]).push([[0],{174:function(e,t,a){e.exports=a(319)},317:function(e,t,a){},319:function(e,t,a){"use strict";a.r(t);var n=a(0),l=a.n(n),r=a(53),i=a.n(r),o=a(47),s=a(48),c=a(50),d=a(49),m=a(74),u=a(322),h=a(323),p=a(172),f=a(171),b=a(100),E=a(13),g=(l.a.Component,a(317),{title:{display:!0,text:"Word2vec-based model training results",fontSize:18}}),v={title:{display:!0,text:"BERT-based model training results",fontSize:18}},y={labels:["ACC","AUC","PRE","REC"],datasets:[{label:"TextCNN",data:[.8616,.9332,.8726,.8503],fill:!1,lineTension:0,borderColor:"#DC143C"},{label:"CharCNN",data:[.8379,.9176,.8383,.8381],fill:!1,lineTension:0,borderColor:"#006400"},{label:"LSTM",data:[.8571,.9201,.8626,.812],fill:!1,lineTension:0,borderColor:"#CD7F32"},{label:"Bi-LSTM",data:[.8451,.9064,.8975,.8029],fill:!1,lineTension:0,borderColor:"#855E42"},{label:"BiLSTM + Attention",data:[.8762,.9381,.9077,.8615],fill:!1,lineTension:0,borderColor:"#FF7F00"}]},w={labels:["ACC","AUC","PRE","REC"],datasets:[{label:"Bi-LSTM",data:[.8841,.9293,.9257,.8592],fill:!1,lineTension:0,borderColor:"#855E42"},{label:"BiLSTM + Attention",data:[.9343,.9506,.9517,.9239],fill:!1,lineTension:0,borderColor:"#FF7F00"}]},C={labels:["Jan","Feb","Mar","Apr","May","Jun"],datasets:[{label:"Number of Tests Conducted Daily",data:[4,5,7,10,9,10],fill:!1,borderColor:"#006400"}]},T={labels:["Jan","Feb","Mar","Apr","May","Jun"],datasets:[{label:"Number of Daily Confirmed Cases",data:[1,3,6,10,20,36],backgroundColor:"#DC143C",borderColor:"DC143C",borderWidth:1,hoverBackgroundColor:"#CD5C5C",hoverBorderColor:"#CD5C5C",display:!0,labelString:"Number of Daily Confirmed Cases",lineHeight:1.2,fontColor:"#666",fontFamily:"'Helvetica Neue', 'Helvetica', 'Arial', sans-serif",fontSize:12,fontStyle:"normal",padding:4}]},N={labels:["Jan","Feb","Mar","Apr","May","Jun"],datasets:[{label:"Number of Daily Tests Conducted Daily",data:[1,3,6,10,20,36],backgroundColor:"#006400",borderColor:"#006400",borderWidth:1,hoverBackgroundColor:"#8FBC8F",hoverBorderColor:"#8FBC8F",display:!0,labelString:"Number of Daily Confirmed Cases",lineHeight:1.2,fontColor:"#666",fontFamily:"'Helvetica Neue', 'Helvetica', 'Arial', sans-serif",fontSize:12,fontStyle:"normal",padding:4}]},A={datasets:[{data:[10,20,30],backgroundColor:["#DC143C","#006400","#00008B","#E7E9ED","#36A2EB"],label:"Effects of Govt Orders in June 2020, IL"}],labels:["Number of Cases","Tests Conducted","Unemployment Claims"]},B=["Project Overview","Motivation","Methods","Results"],M=[{name:B[0],isActive:!0},{name:B[1],isActive:!1},{name:B[2],isActive:!1},{name:B[3],isActive:!1}],S=function(e){Object(c.a)(a,e);var t=Object(d.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(s.a)(a,[{key:"render",value:function(){return l.a.createElement("ul",{className:"nav nav-tabs"},M.map(function(e){return l.a.createElement(k,{data:e,isActive:this.props.activeTab===e,handleClick:this.props.changeTab.bind(this,e)})}.bind(this)))}}]),a}(l.a.Component),k=function(e){Object(c.a)(a,e);var t=Object(d.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(s.a)(a,[{key:"render",value:function(){return l.a.createElement("li",{onClick:this.props.handleClick,className:this.props.isActive?"active":null},l.a.createElement("a",{href:"#"},this.props.data.name))}}]),a}(l.a.Component),x=function(e){Object(c.a)(a,e);var t=Object(d.a)(a);function a(e){var n;return Object(o.a)(this,a),(n=t.call(this,e)).state={error:null,isLoaded:!1,cases:[],tested:[],date:[],daily_confirmed:[]},n}return Object(s.a)(a,[{key:"componentDidMount",value:function(){var e=this;fetch("http://127.0.0.1:5000/case19",{mode:"cors"}).then((function(e){return e.json()})).then((function(t){for(var a=[],n=[],l=[],r=[],i=[],o=0;o<t.length;o++)a.push(t[o].case),n.push(t[o].date.substr(4,2)+"-"+t[o].date.substr(6,2)),l.push(t[o].positiveIncrease),r.push(t[o].totalTestResults),i.push(t[o].totalTestResultsIncrease);e.setState({isLoaded:!0,cases:a,date:n,tested:r,daily_confirmed:l,daily_tested:i})}),(function(t){e.setState({isLoaded:!0,error:t})}))}},{key:"render",value:function(){var e=this.state,t=(e.error,e.isLoaded,e.cases),a=e.date,n=e.daily_confirmed,r=e.tested,i=e.daily_tested;return T.datasets[0].data=n,T.labels=a,C.labels=a,C.datasets[0].data=i,N.labels=a,N.datasets[0].data=i,A.datasets[0].data=[t[t.length-1],r[r.length-1],2570646],l.a.createElement(u.a,{className:"content"},this.props.activeTab.name===B[0]?l.a.createElement(u.a,{fluid:!0},l.a.createElement("section",{className:"panel panel-danger"},l.a.createElement("h2",{className:"panel-heading"},"Project Overview"),l.a.createElement("p",{className:"panel-body"},l.a.createElement("p",null,"The prevalence of the internet greatly propagates the amount of user-generated contents. As user-generated contents usually reflect the thinkings of the authors, they contain valuable information that can be helpful to multiple stakeholders. For example, a movie producer might want to know how audiences are reacting to her latest movie by reading through the comments and reviews online. It is reasonable to argue that such an approach is not efficient enough if there are thousands of reviews available, and it is natural to think of whether it is possible to use deep learning techniques to systematically analyze such information on a large scale."),l.a.createElement("p",null,"Hence, for this project, we propose to test the performance of various deep learning models on predicting sentiments of movie reviews, and we want to focus on short reviews in particular because they are usually hard to train due to the limited length. There has been a lot of research around classifying sentiments of text. In 2002, Peter Turney proposed a simple unsupervised learning algorithm that can classify opinion words as positive or negative with 74% accuracy. The term sentiment analysis first appeared in a 2003 paper by Tetsuya Nasukawa and Jeonghee Yi, where they defined sentiment analysis as to determine how emotions are expressed in the text and whether these expressions indicate  positive or negative opinions on the subject. In 2015, Tai et al. proposed to analyze semantic representations by a serialized LSTM model with added syntactic structure. Their work achieved good results in sentence-level sentiment classification, yet we think there are areas of improvement on their work because the LSTM model only preserves past information. On the other hand, the Bi-LSTM model can preserve both past and future information, and we think this can help to increase accuracy even more. We also want to further explore whether combining models (e.g. Bi-LSTM + Attention) can further improve accuracy of labeling sentiment of short movie review. Therefore, in this project, we construct variations of Bi-LSTM models and compare their performances with other popular NLP models.")),l.a.createElement("center",null,l.a.createElement("div",{className:"vis"},l.a.createElement(u.a,{fluid:!0},l.a.createElement("div",{className:"spacing"}),l.a.createElement(m.a,{id:"line_graph",data:y,options:g}),l.a.createElement("div",{className:"graphspacing"}),l.a.createElement(m.a,{id:"line_graph_cases",data:w,options:v}),l.a.createElement("div",{className:"graphspacing"})))))):null,this.props.activeTab.name===B[1]?l.a.createElement(u.a,{fluid:!0},l.a.createElement("section",{className:"panel panel-danger"},l.a.createElement("h2",{className:"panel-heading"},"Motivation"),l.a.createElement("p",{className:"panel-body"},"The prevalence of the internet greatly propagates the amount of user-generated contents. As user-generated contents usually reflect the thinkings of the authors, they contain valuable information that can be helpful to multiple stakeholders. For example, a movie producer might want to know how audiences are reacting to her latest movie by reading through the comments and reviews online. It is reasonable to argue that such an approach is not efficient enough if there are thousands of reviews available, and it is natural to think of whether it is possible to use deep learning techniques to systematically analyze such information on a large scale. Hence, for this project, we propose to test the performance of various deep learning models on predicting sentiments of movie reviews, and we want to focus on short reviews in particular because they are usually hard to train due to the limited length. Another challenge of analyzing short reviews is that they often contain a lot of verbal language and may not be written in standard English grammar."))):null,this.props.activeTab.name===B[2]?l.a.createElement(u.a,{fluid:!0},l.a.createElement("section",{className:"panel panel-danger"},l.a.createElement("h2",{className:"panel-heading"},"Methods"),l.a.createElement("p",{className:"panel-body"},l.a.createElement("p",null,"We train our models on the IMDB movie sentiment dataset provided by Maas et al.. The dataset contains 50,000 binary labeled movie reviews for training and testing, with the reviews equally divided into training and testing sets. 50,000 unlabeled data are also included for unsupervised training."),l.a.createElement("p",null,"We analyze the dataset by examining the distribution of text length. The majority of reviews are within the length of 250 words. As we focus on short movie reviews for the project, we decide to use 200 as the maximum text length to filter out long reviews. We then perform data cleansing for feature construction by decapitalizing all the letters and removing punctuations and stopwords. We also remove words with low frequency because they may be typos and do not carry significant meaning."),l.a.createElement("p",null,"Below is the distribution of our dataset."),l.a.createElement("center",null,l.a.createElement("img",{className:"body-image",src:"https://raw.githubusercontent.com/EmilyCheoh/deep_learning/main/src/data.png",alt:"method"}))),l.a.createElement("p",{className:"panel-body"},l.a.createElement("p",null,"After the preprocessing is done, we experiment with two different models to generate word embeddings that are later used as the input of our deep learning models. We first use the Word2vec model. We specify to use the Skip-gram method to train, which predicts the word based on relevant context. We choose Skip-gram over CBOW because it is good at representing rare words and has higher accuracy."),l.a.createElement("p",null,"We also use the BERT model to generate sentence embeddings as it is a relatively new model and is good at resolving polysemy, so we wonder if using word embeddings from the BERT model can improve accuracy. We use the BERT base model with 12 layers. Although it is possible to use the output of any layer as the word embeddings for later use, we find that the output of the 11th layer produces the best results. If we use the output from earlier layers, the model may not be sufficiently trained; if we directly use the output from the last layer, it would be too similar to the original text. "),l.a.createElement("p",null,"We then construct a Bi-LSTM model with 128 neurons for training, with each neuron defining a forward LSTM structure and a reverse LSTM structure. We then concatenate their outputs, and pass it to the next layer of the Bi-LSTM model. "),l.a.createElement("p",null,"To examine whether combining the Attention model with Bi-LSTM can improve accuracy, we pass the result of the last layer of the Bi-LSTM model to an Attention model. We then apply the tanh activation function to the output, multiply it with the weight vector, calculate the softmax, and pass the final result to the fully connected layer."),l.a.createElement("center",null,l.a.createElement("img",{className:"body-image",src:"https://raw.githubusercontent.com/EmilyCheoh/deep_learning/main/src/flowchart.png",alt:"method"}))))):null,this.props.activeTab.name===B[3]?l.a.createElement(u.a,{fluid:!0},l.a.createElement("section",{className:"panel panel-danger"},l.a.createElement("h2",{className:"panel-heading"},"Results"),l.a.createElement("p",{className:"panel-body"},l.a.createElement("p",null,"Because we're performing binary classification, our predicting results belong to one of the following four outcomes: true positive, true negative), false positive, and false negative. Therefore, we measure performance by four metrics: Area under the ROC Curve (AUC), Accuracy (ACC), precision (PRE), and recall (REC). Ideally, we would like to maximize all of the above metrics. "),l.a.createElement("p",null,"We compare both our Word2Vec-based model and BERT-based model against four other commonly used deep learning NLP models such as textCNN and CharCNN, as well as a single directional LSTM model. The results are shown in the following table:"),l.a.createElement("center",null,l.a.createElement("table",{className:"results"},l.a.createElement("tr",null,l.a.createElement("th",null,"Word2vec-based Models"),l.a.createElement("th",null,"ACC"),l.a.createElement("th",null,"AUC"),l.a.createElement("th",null,"PRE"),l.a.createElement("th",null,"REC")),l.a.createElement("tr",null,l.a.createElement("td",null,"TextCNN"),l.a.createElement("td",null,"0.8616"),l.a.createElement("td",null,"0.9332"),l.a.createElement("td",null,"0.8726"),l.a.createElement("td",null,"0.8503")),l.a.createElement("tr",null,l.a.createElement("td",null,"CharCNN"),l.a.createElement("td",null,"0.8379"),l.a.createElement("td",null,"0.9176"),l.a.createElement("td",null,"0.8383"),l.a.createElement("td",null,"0.8381")),l.a.createElement("tr",null,l.a.createElement("td",null,"LSTM"),l.a.createElement("td",null,"0.8571"),l.a.createElement("td",null,"0.9201"),l.a.createElement("td",null,"0.8626"),l.a.createElement("td",null,"0.8120")),l.a.createElement("tr",null,l.a.createElement("td",null,"Bi-LSTM"),l.a.createElement("td",null,"0.8451"),l.a.createElement("td",null,"0.9064"),l.a.createElement("td",null,"0.8975"),l.a.createElement("td",null,"0.8029")),l.a.createElement("tr",null,l.a.createElement("td",null,"Bi-LSTM+Attention"),l.a.createElement("td",null,"0.8762"),l.a.createElement("td",null,"0.9381"),l.a.createElement("td",null,"0.9077"),l.a.createElement("td",null,"0.8615")))),l.a.createElement("br",null),l.a.createElement("center",null,l.a.createElement("table",{className:"results"},l.a.createElement("tr",null,l.a.createElement("th",null,"BERT-based Models \xa0 \xa0 \xa0 \xa0"),l.a.createElement("th",null,"ACC"),l.a.createElement("th",null,"AUC"),l.a.createElement("th",null,"PRE"),l.a.createElement("th",null,"REC")),l.a.createElement("tr",null,l.a.createElement("td",null,"Bi-LSTM"),l.a.createElement("td",null,"0.8841"),l.a.createElement("td",null,"0.9293"),l.a.createElement("td",null,"0.9257"),l.a.createElement("td",null,"0.8592")),l.a.createElement("tr",null,l.a.createElement("td",null,"Bi-LSTM+Attention"),l.a.createElement("td",null,"0.9343"),l.a.createElement("td",null,"0.9506"),l.a.createElement("td",null,"0.9517"),l.a.createElement("td",null,"0.9239")),l.a.createElement("br",null))),l.a.createElement("br",null),l.a.createElement("p",null,"Below are interactive visualizations of our results. Click on the label to show and hide lines for easier comparison."),l.a.createElement("center",null,l.a.createElement("div",{className:"vis"},l.a.createElement(u.a,{fluid:!0},l.a.createElement("div",{className:"spacing"}),l.a.createElement(m.a,{id:"line_graph",data:y,options:g}),l.a.createElement("div",{className:"graphspacing"}),l.a.createElement(m.a,{id:"line_graph_cases",data:w,options:v}),l.a.createElement("div",{className:"graphspacing"}))))))):null)}}]),a}(l.a.Component),L=function(e){Object(c.a)(a,e);var t=Object(d.a)(a);function a(e){var n;return Object(o.a)(this,a),(n=t.call(this)).handleClick=function(e){n.setState({activeTab:e})},n.state={activeTab:M[0]},n}return Object(s.a)(a,[{key:"render",value:function(){return l.a.createElement(u.a,{fluid:!0},l.a.createElement(S,{activeTab:this.state.activeTab,changeTab:this.handleClick}),l.a.createElement(x,{activeTab:this.state.activeTab}))}}]),a}(l.a.Component);function j(){return l.a.createElement(u.a,{fluid:!0,className:"header-image-style"},l.a.createElement(h.a,{className:"text-center transparent-0 header-text"},l.a.createElement(h.a.Body,null,l.a.createElement(h.a.Title,{className:"mainheader-text",as:"h1"},"Testing the Performances of Bi-LSTM + Attention model on Sentiment Analysis"),l.a.createElement("div",{className:"info"},l.a.createElement(h.a.Text,{className:"subheader-text",as:"h4"},"Emily Qiao, Jianzhe Xiao"),l.a.createElement(h.a.Text,{className:"subheader-text",as:"h4"},"contact email: emqiao@gmail.com"),l.a.createElement(h.a.Text,{className:"subheader-text",as:"h4"},"Final project for CS 496 Deep Learning, taught by Professor Bryan Pardo"),l.a.createElement(h.a.Text,{className:"subheader-text link",as:"h4"},l.a.createElement("a",{href:"https://github.com/EmilyCheoh/deep_learning/blob/main/final_paper.pdf"},"Link to final paper"))))),l.a.createElement(u.a,{fluid:!0},l.a.createElement(L,null)))}var O=function(){return l.a.createElement(j,null)};a(318);i.a.render(l.a.createElement(O,null),document.getElementById("root"))}},[[174,1,2]]]);
//# sourceMappingURL=main.41228eae.chunk.js.map